import re
import os
import streamlit as st
from dotenv import load_dotenv

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

from core.models import (
    load_models, 
    get_downloaded_models,
    switch_model_via_sdk, 
    HAS_LMS_SDK
)
from core.loader import process_pdf


load_dotenv()
st.set_page_config(page_title="Private Knowledge Brain", page_icon="ğŸ§ ")
st.title("Private Knowledge Brain")


# ì‚¬ì´ë“œë°” í°íŠ¸ ë° ì¤„ë°”ê¿ˆ ì²˜ë¦¬
st.markdown("""
<style>
    [data-testid="stSidebar"] [data-baseweb="select"] span {
        font-size: 0.9rem !important;
        white-space: normal !important; /* ê¸´ ì´ë¦„ ì¤„ë°”ê¿ˆ í—ˆìš© */
        line-height: 1.2 !important;
        height: auto !important;
    }
    ul[data-testid="stSelectboxVirtualDropdown"] li span {
        font-size: 0.85rem !important;
        font-family: monospace !important;
    }
</style>
""", unsafe_allow_html=True)


# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.header("LM Studio ì„¤ì •")
    
    # 1. URL ì„¤ì •
    lm_studio_url = st.text_input(
        "LM Studio API URL", 
        value=os.getenv("LM_STUDIO_URL", "http://localhost:1234"),
        help="LM Studioì˜ Local Server ì£¼ì†Œ (ê¸°ë³¸ê°’: http://localhost:1234)"
    )

    # 2. ëª¨ë¸ ì„ íƒ ë° ë¡œë“œ (SDK ê¸°ëŠ¥ í†µí•©)
    if HAS_LMS_SDK:
        # 1. ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° (ë”•ì…”ë„ˆë¦¬ í˜•íƒœ)
        raw_list = get_downloaded_models()
        
        if raw_list:
            # 2. { 'í™”ë©´ì—_ë³´ì—¬ì¤„_ì´ë¦„': 'ì‹¤ì œ_ê²½ë¡œ' } í˜•íƒœì˜ ë§µ(Map) ìƒì„±
            # ì´ë¦„ì´ ì¤‘ë³µë  ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ì¸ë±ìŠ¤ë¥¼ ì‚´ì§ ë¶™ì—¬ì£¼ê±°ë‚˜ íŒŒì¼ëª…ì„ ê´„í˜¸ì— ë„£ìŒ
            model_map = {}
            for item in raw_list:
                label = item['label']
                path = item['path']
                
                # í‚¤ ì¤‘ë³µ ë°©ì§€ (ì´ë¯¸ ê°™ì€ ì´ë¦„ì´ ìˆìœ¼ë©´ íŒŒì¼ëª… ì¼ë¶€ ì¶”ê°€)
                if label in model_map:
                    # ì˜ˆ: EXAONE (Q4_K_M.gguf)
                    filename = path.split('/')[-1]
                    label = f"{label} ({filename})"
                
                model_map[label] = path

            # 3. Selectboxì—ëŠ” 'í‚¤(ì´ë¦„)'ë§Œ ë„˜ê²¨ì¤Œ -> UIì— ì ˆëŒ€ ë”•ì…”ë„ˆë¦¬ê°€ ì•ˆ ëœ¸
            selected_label = st.selectbox(
                "ì‚¬ìš©í•  ëª¨ë¸ ì„ íƒ", 
                options=list(model_map.keys()), # ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ë§Œ ì „ë‹¬
                index=0
            )
            
            # 4. ì„ íƒëœ ì´ë¦„ìœ¼ë¡œ ì‹¤ì œ ê²½ë¡œ ì°¾ê¸°
            target_path = model_map[selected_label]
            
            # ëª¨ë¸ ë¡œë“œ ë²„íŠ¼
            if st.button("ëª¨ë¸ ë¡œë“œ ë° ì—°ê²°", use_container_width=True):
                with st.spinner(f"'{selected_label}' ëª¨ë¸ ë¡œë“œ ì¤‘..."):
                    # ì‹¤ì œ ê²½ë¡œëŠ” ì—¬ê¸°ì„œ ì‚¬ìš©
                    ctx, err = switch_model_via_sdk(target_path)
                    
                    if err:
                        st.error(f"ë¡œë“œ ì‹¤íŒ¨: {err}")
                    else:
                        st.session_state.model_id = selected_label
                        st.session_state.detected_ctx = ctx
                        st.success("ë¡œë“œ ì™„ë£Œ!")
                        st.rerun()
        else:
            st.warning("ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.error("SDK ë¯¸ì„¤ì¹˜ (pip install lmstudio)")

    # 3. í˜„ì¬ ì—°ê²° ì •ë³´ í‘œì‹œ (ë ˆí¼ëŸ°ìŠ¤ ìŠ¤íƒ€ì¼)
    current_model = st.session_state.get('model_id', 'Unknown')
    current_ctx = st.session_state.get('detected_ctx', 4096)
    
    if current_model != "Unknown":
        with st.expander("í˜„ì¬ ëª¨ë¸ ì •ë³´", expanded=True):
            st.markdown(f"**ëª¨ë¸ëª…:** `{current_model}`")
            st.markdown(f"**ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸:** `{current_ctx:,}` tokens")
            st.info("SDKë¥¼ í†µí•´ ëª¨ë¸ ì •ë³´ë¥¼ ìë™ìœ¼ë¡œ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")

    st.markdown("---")
    st.header("ëª¨ë¸ íŒŒë¼ë¯¸í„°")

    # 4. íŒŒë¼ë¯¸í„° ì„¤ì •
    temperature = st.slider(
        "Temperature", 
        0.0, 1.0, 0.1, 0.1,
        help="ë†’ì„ìˆ˜ë¡ ì°½ì˜ì , ë‚®ì„ìˆ˜ë¡ ì¼ê´€ì ì…ë‹ˆë‹¤."
    )
    
    # Max Tokens í”„ë¦¬ì…‹ ë²„íŠ¼
    col1, col2, col3 = st.columns(3)
    with col1:
        if st.button("ì§§ê²Œ (512)", use_container_width=True):
            st.session_state.preset_tokens = 512
    with col2:
        if st.button("ë³´í†µ (2K)", use_container_width=True):
            st.session_state.preset_tokens = 2048
    with col3:
        if st.button("ê¸¸ê²Œ (Max)", use_container_width=True):
            st.session_state.preset_tokens = min(8192, current_ctx)

    # í”„ë¦¬ì…‹ ì ìš© ë¡œì§
    default_max = min(2048, current_ctx)
    if 'preset_tokens' in st.session_state:
        default_max = st.session_state.preset_tokens
        # ìŠ¬ë¼ì´ë” ê°’ ë°˜ì˜ì„ ìœ„í•´ session state ì •ë¦¬ (ì„ íƒì‚¬í•­)
        del st.session_state.preset_tokens
        st.rerun()

    max_tokens = st.slider(
        "Max Tokens (ì¶œë ¥ ê¸¸ì´)", 
        128, 
        current_ctx, 
        default_max,
        128,
        help=f"í˜„ì¬ ëª¨ë¸ì˜ ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸: {current_ctx:,} tokens"
    )
    
    top_k = st.slider(
        "ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ (Top-K)", 
        1, 10, 5, 1,
        help="ë¬¸ì„œì—ì„œ ê°€ì ¸ì˜¬ ì°¸ì¡° ì²­í¬ì˜ ê°œìˆ˜ì…ë‹ˆë‹¤."
    )
    
    st.markdown("---")
    st.header("ë¬¸ì„œ ì—…ë¡œë“œ")
    uploaded_file = st.file_uploader("PDF íŒŒì¼", type="pdf")

    st.markdown("---")
    
    # 5. ë„ì›€ë§ ë° ì •ë³´ (ë ˆí¼ëŸ°ìŠ¤ ë‚´ìš© ë³µì›)
    with st.expander("ì‚¬ìš© ë°©ë²•"):
        st.markdown("""
        **1ë‹¨ê³„: ëª¨ë¸ ì¤€ë¹„**
        - ìœ„ ëª©ë¡ì—ì„œ ëª¨ë¸ì„ ì„ íƒí•˜ê³  **'ëª¨ë¸ ë¡œë“œ ë° ì—°ê²°'** ë²„íŠ¼ì„ ëˆ„ë¥´ì„¸ìš”.
        - LM Studioê°€ ì‹¤í–‰ ì¤‘ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
        
        **2ë‹¨ê³„: ì„¤ì • í™•ì¸**
        - 'í˜„ì¬ ëª¨ë¸ ì •ë³´'ì—ì„œ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•˜ì„¸ìš”.
        
        **3ë‹¨ê³„: ë¬¸ì„œ ì—…ë¡œë“œ**
        - PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ìë™ìœ¼ë¡œ ë¶„ì„ì´ ì‹œì‘ë©ë‹ˆë‹¤.
        
        **4ë‹¨ê³„: ì§ˆë¬¸í•˜ê¸°**
        - ì±„íŒ…ì°½ì— ë…¼ë¬¸ ë‚´ìš©ì„ ì§ˆë¬¸í•˜ê±°ë‚˜ ìš”ì•½ì„ ìš”ì²­í•˜ì„¸ìš”.
        """)
    
    with st.expander("ëª¨ë¸ì´ ëª©ë¡ì— ì•ˆ ë³´ì´ë‚˜ìš”?"):
        st.markdown("""
        **LM Studio í™•ì¸:**
        1. LM Studioì˜ 'My Models' í´ë”ì— ëª¨ë¸ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.
        2. LM Studio í”„ë¡œê·¸ë¨ì´ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.
        3. `pip install lmstudio`ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.
        """)
    
    st.info("Windows ë¡œì»¬ í™˜ê²½")


llm, embeddings = load_models(lm_studio_url, temperature, max_tokens)



# ==========================================
# ë©”ì¸ ë¡œì§: ë°ì´í„° ì²˜ë¦¬ ë° ì±„íŒ…
# ==========================================

if "messages" not in st.session_state: st.session_state.messages = []
if "retriever" not in st.session_state: st.session_state.retriever = None
if "full_text" not in st.session_state: st.session_state.full_text = None

# PDF ì²˜ë¦¬
if uploaded_file and (st.session_state.retriever is None or st.session_state.full_text is None):
    with st.spinner("PDF ë¶„ì„ ì¤‘..."):
        # process_pdfëŠ” (retriever, full_text) ë‘ ê°œë¥¼ ë°˜í™˜í•´ì•¼ í•¨
        retriever, full_text = process_pdf(
            uploaded_file.getvalue(), embeddings, top_k
        )
        st.session_state.retriever = retriever
        st.session_state.full_text = full_text
    st.success("ë¶„ì„ ì™„ë£Œ!")

# ì±„íŒ… íˆìŠ¤í† ë¦¬
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# ì‚¬ìš©ì ì…ë ¥
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    if not st.session_state.retriever:
        st.error("PDFë¥¼ ë¨¼ì € ì—…ë¡œë“œí•˜ì„¸ìš”")
    else:
        st.chat_message("user").markdown(prompt)
        st.session_state.messages.append({"role": "user", "content": prompt})

        # ==========================================
        # [NEW] LLM Router: ì˜ë„ ë¶„ë¥˜
        # ==========================================
        # ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ íŒŒì•…í•˜ëŠ” ê°€ë²¼ìš´ ì²´ì¸
        router_template = """Task: Classify the user's question into 'SUMMARY' or 'SEARCH'.
        
        Rules:
        1. "SUMMARY": Broad questions, summaries, overviews, main topics.
        2. "SEARCH": Specific facts, numbers, page lookup, definitions.
        3. Do NOT generate <think> tags or reasoning. 
        4. Output ONLY the class name.

        Question: {question}
        Class:"""
        
        router_chain = ChatPromptTemplate.from_template(router_template) | llm | StrOutputParser()
        
        # UIì— ë¶„ë¥˜ ê²°ê³¼ í‘œì‹œ (ë””ë²„ê¹…ìš©, ì›ì¹˜ ì•Šìœ¼ë©´ ì£¼ì„ ì²˜ë¦¬)
        with st.status("ì§ˆë¬¸ ë¶„ì„ ì¤‘...", expanded=False) as status:
            raw_intent = router_chain.invoke({"question": prompt}).strip().upper()
            clean_intent = re.sub(r'<think>.*?</think>', '', raw_intent, flags=re.DOTALL).strip().upper()
        
            # í…ìŠ¤íŠ¸ì— SUMMARYë‚˜ SEARCHê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸ (ë” ì•ˆì „í•˜ê²Œ)
            intent = "SUMMARY" if "SUMMARY" in clean_intent else "SEARCH"
            status.update(label=f"ì§ˆë¬¸ ìœ í˜• ê°ì§€: {intent}", state="complete")

        # Context ì„¤ì • (Routing ê²°ê³¼ ì ìš©)
        context_data = ""
        source_info = ""

        if "SUMMARY" in intent:
            # [SUMMARY ëª¨ë“œ] ì „ì²´ í…ìŠ¤íŠ¸ ì‚¬ìš©
            # ëª¨ë¸ Context Limitì—ì„œ Output í† í°ê³¼ ì—¬ìœ ë¶„ì„ ëº€ ë§Œí¼ë§Œ ì…ë ¥
            safe_limit = st.session_state.detected_ctx - max_tokens - 500
            # í•œê¸€/ì˜ì–´ ì„ì„ ê³ ë ¤í•˜ì—¬ ëŒ€ëµ 3ë°°ìˆ˜ë¡œ ìë¦„ (ë‹¨ìˆœí™”ëœ ë¡œì§)
            char_limit = int(safe_limit * 2.5)
            
            context_data = st.session_state.full_text[:char_limit]
            source_info = f"\n\n*( ì „ì²´ ë¬¸ì„œ ë¶„ì„ ëª¨ë“œ | Context: {safe_limit} tokens )*"
        else:
            # [SEARCH ëª¨ë“œ] RAG ê²€ìƒ‰ ì‚¬ìš©
            docs = st.session_state.retriever.invoke(prompt)
            context_data = "\n\n".join([f"[Page {d.metadata.get('page','?')}] {d.page_content}" for d in docs])
            source_info = "\n\n*( ì •ë°€ ê²€ìƒ‰ ëª¨ë“œ )*"
        
        
        
        # ë‹µë³€ ìƒì„±    
        template = """ë‹¹ì‹ ì€ ë…¼ë¬¸ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë¬¸ë§¥ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

[ì œì•½ ì¡°ê±´]
1. ì œê³µëœ ë¬¸ë§¥(Context)ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
2. ì „ë¬¸ ìš©ì–´ëŠ” ì˜ì–´ ì›ë¬¸ì„ ìœ ì§€í•˜ì„¸ìš” (ì˜ˆ: 'Diffusion Model', 'Attention Mechanism').
3. ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ë˜, í•„ìš”ì‹œ ì˜ì–´ ìš©ì–´ë¥¼ ë³‘ê¸°í•˜ì„¸ìš”.
4. ë¬¸ë§¥ì—ì„œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´ "ì œê³µëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”.
5. ë‹µë³€ ë§ˆì§€ë§‰ì— ì°¸ê³ í•œ í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ëª…ì‹œí•˜ì„¸ìš”.

[ë¬¸ë§¥]:
{context}

[ì§ˆë¬¸]: {question}

[ë‹µë³€]:"""
        
        chain = (
            {"context": lambda x: context_data, "question": RunnablePassthrough()}
            | ChatPromptTemplate.from_template(template) 
            | llm 
            | StrOutputParser()
        )

        with st.chat_message("assistant"):
            # ë‘ ê°œì˜ ì˜ì—­ ì¤€ë¹„: ì‚¬ê³  ê³¼ì •(Expander) + ìµœì¢… ë‹µë³€(Main)
            reasoning_area = st.empty()
            answer_area = st.empty()
            
            # ìƒíƒœ ë³€ìˆ˜
            full_response = ""       # ì „ì²´ ë¡œê·¸ ì €ì¥ìš©
            reasoning_content = ""   # ì‚¬ê³  ê³¼ì • í…ìŠ¤íŠ¸
            answer_content = ""      # ìµœì¢… ë‹µë³€ í…ìŠ¤íŠ¸
            is_thinking = False      # í˜„ì¬ ì‚¬ê³  ì¤‘ì¸ê°€?
            
            try:
                for chunk in chain.stream(prompt):
                    full_response += chunk

                    # [State Machine] íƒœê·¸ ê°ì§€ ë° ëª¨ë“œ ì „í™˜
                    # 1. ì‚¬ê³  ì‹œì‘ ê°ì§€ (<think>)
                    if "<think>" in chunk:
                        is_thinking = True
                        chunk = chunk.replace("<think>", "")
                        
                        # UI: ì‚¬ê³  ê³¼ì • ì˜ì—­ ìƒì„±
                        with reasoning_area.container():
                            with st.expander("ğŸ’­ ì‚¬ê³  ê³¼ì • (Thinking Process)", expanded=True):
                                reasoning_placeholder = st.empty()
                    
                    # 2. ì‚¬ê³  ì¢…ë£Œ ê°ì§€ (</think>)
                    if "</think>" in chunk:
                        is_thinking = False
                        chunk = chunk.replace("</think>", "")
                        
                        # UI: ì‚¬ê³  ê³¼ì • ì™„ë£Œ ìƒíƒœë¡œ ì—…ë°ì´íŠ¸ (ì ‘íŒ ìƒíƒœë¡œ ë°”ê¾¸ê±°ë‚˜ ìœ ì§€)
                        with reasoning_area.container():
                            with st.expander("ğŸ’­ ì‚¬ê³  ê³¼ì • (Thinking Process)", expanded=False):
                                st.markdown(reasoning_content)
                    

                    # [Display] ëª¨ë“œì— ë”°ë¥¸ ì¶œë ¥ ìœ„ì¹˜ ê²°ì •
                    if is_thinking:
                        reasoning_content += chunk
                        # expander ë‚´ë¶€ placeholder ì—…ë°ì´íŠ¸
                        try:
                            reasoning_placeholder.markdown(reasoning_content + "â–Œ")
                        except:
                            pass
                    else:
                        answer_content += chunk
                        answer_area.markdown(answer_content + "â–Œ")

                # ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ í›„ ë§ˆë¬´ë¦¬ (ì»¤ì„œ ì œê±° ë° ì¶œì²˜ ë¶€ì°©)
                answer_area.markdown(answer_content + source_info)
                
                # ì‚¬ê³  ê³¼ì •ì´ ìˆì—ˆë˜ ê²½ìš°, ìµœì¢…ì ìœ¼ë¡œ ê¹”ë”í•˜ê²Œ ë Œë”ë§
                if reasoning_content:
                    reasoning_area.empty() # ê¸°ì¡´ placeholder ì œê±°
                    with reasoning_area.container():
                        with st.expander("ğŸ’­ ì‚¬ê³  ê³¼ì • (Thinking Process)", expanded=False):
                            st.markdown(reasoning_content)

            except Exception as e:
                st.error(f"Error: {e}")
        
        # íˆìŠ¤í† ë¦¬ì—ëŠ” 'ìµœì¢… ë‹µë³€'ë§Œ ì €ì¥í• ì§€, 'ì‚¬ê³  ê³¼ì •'ë„ í¬í•¨í• ì§€ ê²°ì •
        # ë³´í†µì€ ê¹”ë”í•˜ê²Œ ìµœì¢… ë‹µë³€ë§Œ ì €ì¥í•˜ê±°ë‚˜, í¬ë§·íŒ…í•´ì„œ ì €ì¥í•¨
        final_save_content = answer_content + source_info
        
        # (ì„ íƒì‚¬í•­) íˆìŠ¤í† ë¦¬ì—ì„œë„ ì‚¬ê³  ê³¼ì •ì„ ë³´ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
        # if reasoning_content:
        #     final_save_content = f"<details><summary>ì‚¬ê³  ê³¼ì •</summary>{reasoning_content}</details>\n\n" + final_save_content
            
        st.session_state.messages.append({"role": "assistant", "content": final_save_content})

# ì±„íŒ… ì´ˆê¸°í™”
if st.session_state.messages:
    if st.sidebar.button("ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”"):
        st.session_state.messages = []
        st.rerun()