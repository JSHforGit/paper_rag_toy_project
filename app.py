import streamlit as st
import os
import re
import tempfile
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# ==========================================
# 1. ì„¤ì • ë° ìºì‹± (ì„±ëŠ¥ ìµœì í™”)
# ==========================================
st.set_page_config(page_title="Private Knowledge Brain", page_icon="ğŸ§ ")
st.title("ğŸ§  Private Knowledge Brain (LM Studio)")

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.header("âš™ï¸ LM Studio ì„¤ì •")
    lm_studio_url = st.text_input(
        "LM Studio API URL", 
        value="http://localhost:1234/v1",
        help="LM Studioì˜ Local Server ì£¼ì†Œ (ê¸°ë³¸ê°’: http://localhost:1234/v1)"
    )
    
    # ê³ ê¸‰ ì„¤ì •
    with st.expander("ğŸ›ï¸ ëª¨ë¸ íŒŒë¼ë¯¸í„°"):
        temperature = st.slider("Temperature", 0.0, 1.0, 0.1, 0.1)
        max_tokens = st.slider("Max Tokens", 128, 2048, 512, 128)
        top_k = st.slider("Top-K (ê²€ìƒ‰ ê²°ê³¼ ìˆ˜)", 1, 10, 5, 1)
    
    # LM Studio ì—°ê²° ìƒíƒœ í™•ì¸
    if st.button("ğŸ”Œ ì—°ê²° í…ŒìŠ¤íŠ¸"):
        try:
            import requests
            response = requests.get(
                f"{lm_studio_url.replace('/v1', '')}/v1/models", 
                timeout=3
            )
            if response.status_code == 200:
                models = response.json()
                model_list = models.get('data', [])
                st.success(f"âœ… ì—°ê²° ì„±ê³µ!")
                if model_list:
                    st.info(f"ğŸ“¦ ë¡œë“œëœ ëª¨ë¸:\n{model_list[0].get('id', 'Unknown')}")
                else:
                    st.warning("âš ï¸ ë¡œë“œëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. LM Studioì—ì„œ ëª¨ë¸ì„ ë¡œë“œí•˜ì„¸ìš”.")
            else:
                st.error("âŒ ì—°ê²° ì‹¤íŒ¨")
        except Exception as e:
            st.error(f"âŒ ì—°ê²° ë¶ˆê°€: {str(e)}")
            st.info("ğŸ’¡ LM Studioì—ì„œ 'Start Server'ë¥¼ ëˆŒë €ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    
    st.markdown("---")
    st.header("ğŸ“„ Upload Document")
    uploaded_file = st.file_uploader("PDF íŒŒì¼ì„ ì˜¬ë ¤ì£¼ì„¸ìš”", type="pdf")
    st.markdown("---")
    
    # ì‚¬ìš© ê°€ì´ë“œ
    with st.expander("ğŸ“– ì‚¬ìš© ë°©ë²•"):
        st.markdown("""
        **1ë‹¨ê³„: LM Studio ì¤€ë¹„**
        - LM Studio ì‹¤í–‰
        - ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ
        - 'Local Server' íƒ­ì—ì„œ 'Start Server' í´ë¦­
        
        **2ë‹¨ê³„: ë¬¸ì„œ ì—…ë¡œë“œ**
        - ì™¼ìª½ì—ì„œ PDF íŒŒì¼ ì—…ë¡œë“œ
        
        **3ë‹¨ê³„: ì§ˆë¬¸í•˜ê¸°**
        - ì•„ë˜ ì±„íŒ…ì°½ì—ì„œ ì§ˆë¬¸ ì…ë ¥
        """)
    
    st.info("ğŸ’» Windows ë¡œì»¬ í™˜ê²½ì—ì„œ ì‹¤í–‰ ì¤‘")

# ëª¨ë¸ ë¡œë“œ (ìºì‹±í•˜ì—¬ ë§¤ë²ˆ ë¡œë”©í•˜ì§€ ì•Šë„ë¡ í•¨)
@st.cache_resource
def load_llm_and_embeddings(_lm_studio_url, _temperature, _max_tokens):
    """LLM ë° ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"""
    # ì„ë² ë”© ëª¨ë¸ (ë¡œì»¬ì—ì„œ ì‹¤í–‰)
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={'device': 'cpu'},  # Windowsì—ì„œ ì•ˆì •ì ì¸ CPU ì‚¬ìš©
        encode_kwargs={'normalize_embeddings': True}
    )
    
    # LM Studio LLM (OpenAI í˜¸í™˜)
    llm = ChatOpenAI(
        base_url=_lm_studio_url,
        api_key="lm-studio",  # LM StudioëŠ” dummy key ì‚¬ìš©
        temperature=_temperature,
        max_tokens=_max_tokens,
        streaming=True  # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
    )
    return llm, embeddings

llm, embeddings = load_llm_and_embeddings(lm_studio_url, temperature, max_tokens)

# ==========================================
# 2. ë°ì´í„° ì²˜ë¦¬ ë¡œì§
# ==========================================
def is_garbage(text):
    """ë…¸ì´ì¦ˆ í…ìŠ¤íŠ¸ í•„í„°ë§"""
    if len(text) < 100: 
        return True
    num_count = len(re.findall(r'\d', text))
    if num_count / len(text) > 0.2: 
        return True
    return False

@st.cache_data
def process_pdf(file_bytes, _embeddings, _top_k):
    """PDF ì²˜ë¦¬ ë° ê²€ìƒ‰ê¸° ìƒì„±"""
    # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(file_bytes)
        tmp_path = tmp_file.name

    # PDF ë¡œë“œ
    loader = PyPDFLoader(tmp_path)
    pages = loader.load()
    
    # í…ìŠ¤íŠ¸ ë¶„í• 
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500, 
        chunk_overlap=100,
        separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""]
    )
    raw_splits = text_splitter.split_documents(pages)
    
    # ì •ì œ (Garbage Collection)
    clean_splits = [doc for doc in raw_splits if not is_garbage(doc.page_content)]
    
    st.info(f"ğŸ“Š ì´ {len(clean_splits)}ê°œì˜ í…ìŠ¤íŠ¸ ì²­í¬ ìƒì„±ë¨")
    
    # ë²¡í„° DB & ê²€ìƒ‰ê¸° ìƒì„±
    vectorstore = Chroma.from_documents(
        documents=clean_splits, 
        embedding=_embeddings,
        persist_directory=None  # ë©”ëª¨ë¦¬ì—ë§Œ ì €ì¥ (ë¹ ë¥¸ ì²˜ë¦¬)
    )
    chroma_retriever = vectorstore.as_retriever(search_kwargs={"k": _top_k})
    
    bm25_retriever = BM25Retriever.from_documents(clean_splits)
    bm25_retriever.k = _top_k
    
    # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
    ensemble_retriever = EnsembleRetriever(
        retrievers=[bm25_retriever, chroma_retriever],
        weights=[0.3, 0.7]  # Semantic searchì— ë” ë†’ì€ ê°€ì¤‘ì¹˜
    )
    
    # ì„ì‹œ íŒŒì¼ ì‚­ì œ
    os.unlink(tmp_path)
    
    return ensemble_retriever

# ==========================================
# 3. UI ë° ì±„íŒ… ë¡œì§
# ==========================================
if "messages" not in st.session_state:
    st.session_state.messages = []

if "retriever" not in st.session_state:
    st.session_state.retriever = None

# PDF ì—…ë¡œë“œ ì²˜ë¦¬
if uploaded_file and st.session_state.retriever is None:
    with st.spinner("ğŸ“„ PDFë¥¼ ë¶„ì„í•˜ê³  ì¸ë±ì‹±í•˜ëŠ” ì¤‘... (ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”)"):
        file_bytes = uploaded_file.getvalue()
        st.session_state.retriever = process_pdf(file_bytes, embeddings, top_k)
    st.success("âœ… ë¶„ì„ ì™„ë£Œ! ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")
    st.balloons()

# ì±„íŒ… ê¸°ë¡ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input("ë…¼ë¬¸ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”..."):
    if st.session_state.retriever is None:
        st.error("âŒ ë¨¼ì € PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
    else:
        # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
        st.chat_message("user").markdown(prompt)
        st.session_state.messages.append({"role": "user", "content": prompt})

        # RAG íŒŒì´í”„ë¼ì¸
        def format_docs(docs):
            """ê²€ìƒ‰ëœ ë¬¸ì„œ í¬ë§·íŒ…"""
            return "\n\n".join([
                f"[ì¶œì²˜: Page {d.metadata.get('page', '?')}]\n{d.page_content}" 
                for d in docs
            ])

        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        template = """ë‹¹ì‹ ì€ ë…¼ë¬¸ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë¬¸ë§¥ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

[ì œì•½ ì¡°ê±´]
1. ì œê³µëœ ë¬¸ë§¥(Context)ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
2. ì „ë¬¸ ìš©ì–´ëŠ” ì˜ì–´ ì›ë¬¸ì„ ìœ ì§€í•˜ì„¸ìš” (ì˜ˆ: 'Diffusion Model', 'Attention Mechanism').
3. ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ë˜, í•„ìš”ì‹œ ì˜ì–´ ìš©ì–´ë¥¼ ë³‘ê¸°í•˜ì„¸ìš”.
4. ë¬¸ë§¥ì—ì„œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´ "ì œê³µëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”.
5. ë‹µë³€ ë§ˆì§€ë§‰ì— ì°¸ê³ í•œ í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ëª…ì‹œí•˜ì„¸ìš”.

[ë¬¸ë§¥]:
{context}

[ì§ˆë¬¸]: {question}

[ë‹µë³€]:"""
        
        prompt_template = ChatPromptTemplate.from_template(template)
        
        # RAG Chain êµ¬ì„±
        rag_chain = (
            {
                "context": st.session_state.retriever | format_docs, 
                "question": RunnablePassthrough()
            }
            | prompt_template
            | llm
            | StrOutputParser()
        )
        
        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
        with st.chat_message("assistant"):
            response_placeholder = st.empty()
            full_response = ""
            
            try:
                for chunk in rag_chain.stream(prompt):
                    full_response += chunk
                    response_placeholder.markdown(full_response + "â–Œ")
                
                response_placeholder.markdown(full_response)
                
            except Exception as e:
                error_msg = f"âŒ ì—ëŸ¬ ë°œìƒ: {str(e)}"
                st.error(error_msg)
                
                # ë””ë²„ê¹… ì •ë³´
                with st.expander("ğŸ” ì—ëŸ¬ ìƒì„¸ ì •ë³´"):
                    st.code(str(e))
                    st.markdown("""
                    **í•´ê²° ë°©ë²•:**
                    1. LM Studioì—ì„œ ëª¨ë¸ì´ ë¡œë“œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                    2. LM Studioì˜ 'Local Server'ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
                    3. ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ 'ğŸ”Œ ì—°ê²° í…ŒìŠ¤íŠ¸' ë²„íŠ¼ í´ë¦­
                    4. í¬íŠ¸ ë²ˆí˜¸ê°€ ë§ëŠ”ì§€ í™•ì¸ (ê¸°ë³¸ê°’: 1234)
                    """)
                
                full_response = "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        
        st.session_state.messages.append({
            "role": "assistant", 
            "content": full_response
        })

# ì±„íŒ… ì´ˆê¸°í™” ë²„íŠ¼
if st.session_state.messages:
    if st.sidebar.button("ğŸ—‘ï¸ ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”"):
        st.session_state.messages = []
        st.rerun()